---
title: "llms.txt: Robots.txt for Docs in the AI Era"
last_modified_at: 2025-09-19T0:00:00-05:00
author: Dewan Ahmed
header:
  teaser: "/assets/images/2025/llms-txt.png"
tags:
  - documentation
  - ai
  - devrel
---

AI tools are already crawling your docs.  
By default, you have zero control.  

If you open ChatGPT, Perplexity, or GitHub Copilot today and ask about your product, chances are the answer is coming from **your documentation** — whether you intended it or not.  

Docs aren’t just for humans anymore. They’re also inputs for AI systems. And if you don’t set boundaries, AI will scrape whatever it finds: old drafts, beta endpoints, internal notes.  

The good news is there are simple ways to guide this. Let’s start with `llms.txt` and a few doc patterns that make your content easier for both humans and machines.

## Why control matters now

In the early web, `robots.txt` gave site owners a way to tell search engines what to crawl and what to skip. Without it, search engines indexed everything, including things never meant for public eyes.  

We’re now in that same stage with AI crawlers. Models are scraping hungrily, and unless you signal otherwise, they’ll ingest whatever is available.  

`llms.txt` is the emerging convention to set those signals.

## What is `llms.txt`?

A plain text file you place at the root of your docs domain.  
Crawlers that support it will read the rules and adjust what they ingest.

**Basic example:**

```YAML
User-Agent: *
Disallow: /internal/
Allow: /api/
```

Here we’re saying:

* Don’t crawl `/internal/`.
* Do index `/api/`.

Drop this at `yourdocs.com/llms.txt`, and compliant crawlers will follow it.

## Adding metadata

You can also add hints that describe your docs so AI systems know how to handle them.

**Example:**

```txt
User-Agent: GPTBot
Allow: /docs/
Disallow: /beta/
Content-Type: technical-documentation
Attribution: https://docs.example.com
License: CC-BY-4.0
```

Why this matters:

* **Content-Type**: makes it clear this is technical material, not marketing copy.
* **Attribution**: gives models a canonical link to cite.
* **License**: sets expectations for use.

Lightweight to add, useful down the line.

## Beyond `llms.txt`: Make docs friendlier for AI

Controlling access is one piece. The other is making docs consumable by both humans and machines.

### 1. Complement click commands with `curl`

Humans can click buttons or copy-paste CLI commands. LLMs can’t.
If your docs say *“Click here to create a token”* or *“Run `mycli create-token`”*, an LLM has no idea what to do with that.

So pair human-friendly steps with machine-friendly ones.

```bash
# CLI example for humans
mycli create-token

# Equivalent curl for LLMs and API-level clarity
curl -X POST https://api.example.com/v1/tokens \
  -H "Authorization: Bearer $TOKEN"
```

This way, developers have options, and AI systems have explicit instructions they can reason about.

### 2. Add structured metadata to docs pages

Docs aren’t just prose. Add frontmatter so pages carry context.

```yaml
---
title: "Deploying your first app"
type: tutorial
audience: developer
version: v1.2
---
```

This makes it easier for AI systems (and your own search) to classify and retrieve content accurately.

### 3. Keep repos ingestible

Tools like [gitinjest](https://gitingest.com/) can **turn any Git repository into a simple text digest of its codebase**. That digest can then be fed into an LLM for search, reasoning, or Q\&A.

If your docs live alongside your code, this becomes powerful, but only if the repo is well-structured.

Practical tips:

* Keep `README.md` clean and accurate (it’s usually the first file ingested).
* Organize folders logically (`/api`, `/guides`, `/examples`).
* Include structured specs (OpenAPI, GraphQL, Postman collections) so the digest captures more than prose.

The result: your repo becomes not just developer-friendly, but also **LLM-friendly**, making it easier for AI systems to return correct answers about your product.

### 4. Inline HTML instructions and Vercel’s `llms.txt` REST API

Vercel recently [proposed](https://vercel.com/blog/a-proposal-for-inline-llm-instructions-in-html) a way to embed page-level instructions for LLMs directly in HTML. The idea is to use a `<script type="text/llms.txt">` tag in the `<head>`. Browsers ignore it, but LLMs can read it.

This is useful for things like preview environments or error pages, where you want to tell agents what’s going on without changing what humans see.

```html
<head>
  <script type="text/llms.txt">
This page requires authentication.
Agents should request /auth/bypass with a valid token.
  </script>
</head>
```

Vercel also exposes a [REST API for `llms.txt`](https://vercel.com/docs/rest-api/reference/llms.txt). That means you can generate or update rules programmatically as your docs evolve.

**Example request:**

```http
GET https://api.vercel.com/llms.txt
Authorization: Bearer YOUR_TOKEN
Accept: text/plain
```

**Example response:**

```txt
User-Agent: *
Allow: /docs/
Disallow: /beta/
Content-Type: technical-documentation
Attribution: https://your-org.vercel.com
```

#### Why this helps

* Inline tags let you handle page-specific cases.
* The API lets you automate updates when routes or features change.

#### Trade-offs

* Not all agents respect inline tags.
* More moving parts (auth, caching, consistency) compared to a static file.
* If different pages have different rules, you’ll need conventions to avoid confusion.

## Pitfalls

* Not all crawlers respect `llms.txt` (some ignored `robots.txt` too).
* Old versions may still get picked up unless you block them.
* Machine-friendly doesn’t mean neglecting human readability. Developers are still your first audience.

Think of these steps as guardrails, not hard locks.

## Practical checklist

* ✅ Add `llms.txt` at the root of your docs site
* ✅ Disallow private/beta/legacy paths
* ✅ Add attribution and license metadata
* ✅ Pair click/CLI commands with `curl` examples
* ✅ Add frontmatter metadata to Markdown files
* ✅ Keep repos clean for ingestion tools like `gitinjest`

## Wrapping up

Your docs don’t just serve developers anymore. They also feed the AI systems your developers are already using.

By adding a simple `llms.txt`, pairing human steps with machine-friendly examples, and keeping repos structured, you make your docs more reliable in both worlds.

In the next post, we’ll go deeper: how to **structure and publish AI-ready docs**, so when a developer asks an assistant for help, the answer isn’t just “close.” It’s exactly what your docs say.

For a deeper dive into this shift, I recommend Andrej Karpathy’s [talk on building docs for the AI era](https://www.youtube.com/watch?v=LCEmiRjPEtQ&t=2019s).

